{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Bulk import cell\n",
    "import wandb\n",
    "import random\n",
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from vit_pytorch.mobile_vit import MobileViT\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.nn import functional as F\n",
    "from x_transformers import ViTransformerWrapper, Encoder\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234\n"
     ]
    },
    {
     "data": {
      "text/plain": "1234"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "platform = 'm1'\n",
    "# platform = '2015'\n",
    "\n",
    "\n",
    "\n",
    "if platform == '2015':\n",
    "  dataset_path = '/Users/rotemisraeli/Documents/datasets/TinyImageNet/'\n",
    "elif platform == 'm1':\n",
    "  dataset_path = '/Volumes/black_ssd/datasets/imagenet/imagenet-object-localization-challenge/ILSVRC/Data/images/'\n",
    "\n",
    "pl.seed_everything(1234)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mrotem98\u001B[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "project = \"mobile_test2\"  # W&B project name here\n",
    "entity = 'rotem98'  # your W&B username or teamname here\n",
    "img_size = 192"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class SentenceClassifier(pl.LightningModule):\n",
    "\n",
    "  def __init__(self, learning_rate=5e-5):\n",
    "    super(SentenceClassifier, self).__init__()\n",
    "    # self.model = MobileViT(\n",
    "    #   image_size = (64, 64),\n",
    "    #   dims = [96, 120, 144],\n",
    "    #   channels = [16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 384],\n",
    "    #   num_classes = 100\n",
    "    # )\n",
    "    self.model = ViTransformerWrapper(\n",
    "      image_size = img_size,\n",
    "      patch_size = 16,\n",
    "      num_classes = 1000,\n",
    "      attn_layers = Encoder(\n",
    "          dim = 256,\n",
    "          depth = 4,\n",
    "          heads = 4,\n",
    "          use_qk_norm_attn = True, # set this to True\n",
    "          qk_norm_attn_seq_len = 256 # set this to max_seq_len from above\n",
    "      )\n",
    "    )\n",
    "    self.learning_rate = learning_rate\n",
    "\n",
    "  def training_step(self, batch, batch_no):\n",
    "    input, labels = batch\n",
    "    outputs = self.model(input)\n",
    "    preds = torch.argmax(outputs, axis=1)\n",
    "    correct = sum(preds.flatten() == labels.flatten())\n",
    "    loss = F.cross_entropy(outputs, labels)\n",
    "    self.log(\"train/loss\", loss, on_step=True, on_epoch=True)\n",
    "    self.log(\"train/acc\", correct/len(batch), on_step=True, on_epoch=True)\n",
    "    return loss\n",
    "\n",
    "  def validation_step(self, batch, batch_no):\n",
    "    input, labels = batch\n",
    "    outputs = self.model(input)\n",
    "    preds = torch.argmax(outputs, axis=1)\n",
    "    correct = sum(preds.flatten() == labels.flatten())\n",
    "    loss = F.cross_entropy(outputs, labels)\n",
    "    self.log(\"val/loss\", loss, on_step=False, on_epoch=True)\n",
    "    self.log(\"val/acc\", correct/len(batch), on_step=False, on_epoch=True)\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    return transformers.AdamW(\n",
    "        self.model.parameters(),\n",
    "        lr = self.learning_rate,\n",
    "        eps = 1e-8\n",
    "    )\n",
    "\n",
    "def data_pre(config):\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(img_size),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225] )\n",
    "    ])\n",
    "    val_transform = transforms.Compose([\n",
    "      transforms.Resize(img_size),\n",
    "      transforms.CenterCrop(img_size),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225] )\n",
    "    ])\n",
    "    if platform == 'm1':\n",
    "        dataset = torchvision.datasets.ImageFolder(root=dataset_path+'train', transform=val_transform)\n",
    "        size1 = int(len(dataset)*0.45)\n",
    "        size2 = int(len(dataset)*0.05)\n",
    "        size3 = len(dataset) - size2 - size1\n",
    "        train_set, val_set,not_use_set = torch.utils.data.random_split(dataset,[size1,size2,size3])\n",
    "        train_data_loader = torch.utils.data.DataLoader(train_set, batch_size=config.batch_size, shuffle=True,  num_workers=4)\n",
    "        val_data_loader = torch.utils.data.DataLoader(val_set, batch_size=config.batch_size, shuffle=False,  num_workers=4)\n",
    "    elif platform == '2015':\n",
    "        train_data = torchvision.datasets.ImageFolder(root=dataset_path+'train', transform=train_transform)\n",
    "        train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=config.batch_size, shuffle=True,  num_workers=4)\n",
    "        val_data = torchvision.datasets.ImageFolder(root=dataset_path+'val', transform=val_transform)\n",
    "        val_data_loader = torch.utils.data.DataLoader(val_data, batch_size=config.batch_size, shuffle=False,  num_workers=4)\n",
    "\n",
    "    return train_data_loader,val_data_loader\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def train(config):\n",
    "    with wandb.init(project=project, entity=entity, job_type=\"train\", config=config,dir='wandb_dir') as run:\n",
    "        config = run.config\n",
    "\n",
    "        train_data_loader, val_data_loader = data_pre(config)\n",
    "\n",
    "        model = SentenceClassifier(learning_rate=config.learning_rate)\n",
    "\n",
    "        logger = pl.loggers.WandbLogger(experiment=run, log_model=True,save_dir='wandb_logger_savedir')\n",
    "\n",
    "        gpus = -1 if torch.cuda.is_available() else 0\n",
    "\n",
    "        trainer = pl.Trainer(max_epochs=config.epochs, gpus=gpus, logger=logger,weights_save_path='models')\n",
    "\n",
    "        trainer.fit(model, train_data_loader, val_data_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "wandb: WARNING Path wandb_dir/wandb/ wasn't writable, using system temp directory\n",
      "wandb: WARNING Path wandb_dir/wandb/ wasn't writable, using system temp directory\n",
      "wandb: WARNING Path wandb_dir/wandb/ wasn't writable, using system temp directory\n",
      "wandb: WARNING Path wandb_dir/wandb/ wasn't writable, using system temp directory\n",
      "wandb: WARNING Path wandb_dir/wandb/ wasn't writable, using system temp directory\n",
      "wandb: WARNING Path wandb_dir/wandb/ wasn't writable, using system temp directory\n",
      "wandb: WARNING Path wandb_dir/wandb/ wasn't writable, using system temp directory\n",
      "wandb: WARNING Path wandb_dir/wandb/ wasn't writable, using system temp directory\n",
      "wandb: WARNING Path wandb_dir/wandb/ wasn't writable, using system temp directory\n",
      "wandb: WARNING Path wandb_dir/wandb/ wasn't writable, using system temp directory\n",
      "wandb: WARNING Path wandb_dir/wandb/ wasn't writable, using system temp directory\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.12.15 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.12.11"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/var/folders/jy/n0pbwtwj2qn8qgnc0sqslx300000gn/T/wandb/run-20220501_233355-f7eot6hl</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/rotem98/mobile_test2/runs/f7eot6hl\" target=\"_blank\">brisk-jazz-54</a></strong> to <a href=\"https://wandb.ai/rotem98/mobile_test2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb_dir/wandb/ wasn't writable, using system temp directory.\n",
      "/Users/rotemisraeli/opt/anaconda3/envs/python/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/rotemisraeli/opt/anaconda3/envs/python/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name  | Type                 | Params\n",
      "-----------------------------------------------\n",
      "0 | model | ViTransformerWrapper | 4.7 M \n",
      "-----------------------------------------------\n",
      "4.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.7 M     Total params\n",
      "18.712    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f40253bea964951b7cac4f1f48700fa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Path wandb_dir/wandb/ wasn't writable, using system temp directory\n",
      "wandb: WARNING Path wandb_dir/wandb/ wasn't writable, using system temp directory\n",
      "wandb: WARNING Path wandb_dir/wandb/ wasn't writable, using system temp directory\n",
      "wandb: WARNING Path wandb_dir/wandb/ wasn't writable, using system temp directory\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "981ce4ffb8594ea5a36579758214472f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "train(config={\"learning_rate\": 5e-5, \"batch_size\": 64, \"epochs\": 10})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}